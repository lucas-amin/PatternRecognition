{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa os pacotes necessários\n",
    "import numpy as np\n",
    "import os, cv2, random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import os\n",
    "import np_utils\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funções de leitura e preparação das imagens\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.uint8)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image.T\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))    \n",
    "    return data\n",
    "    \n",
    "# dois exemplos de descritores. Você deve criar outros mais robustos.\n",
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "    # resize the image to a fixed size, then flatten the image into\n",
    "    # a list of raw pixel intensities\n",
    "    return cv2.resize(image, size).flatten()\n",
    "\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):     \n",
    "    # extract a 3D color histogram from the HSV color space using\n",
    "    # the supplied number of `bins` per channel\n",
    "    #image = cv2.imread(image_file)        \n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n",
    "        [0, 180, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    # return the flattened histogram as the feature vector\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'kaggle/cifar-10/train'\n",
    "TRAIN_LABELS_DIR = 'kaggle/cifar-10/trainLabels.csv'\n",
    "\n",
    "ROWS = 128\n",
    "COLS = 128\n",
    "CHANNELS = 3\n",
    "NIM = 10000\n",
    "\n",
    "image_paths = os.listdir(TRAIN_DIR)\n",
    "\n",
    "train_images = [read_image(TRAIN_DIR + \"/\" + i) for i in image_paths][:NIM]\n",
    "train_labels = pd.read_csv(TRAIN_LABELS_DIR)\n",
    "\n",
    "train_labels = train_labels[:NIM]\n",
    "\n",
    "del train_labels['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 10000\n",
      "Processed 1000 of 10000\n",
      "Processed 2000 of 10000\n",
      "Processed 3000 of 10000\n",
      "Processed 4000 of 10000\n",
      "Processed 5000 of 10000\n",
      "Processed 6000 of 10000\n",
      "Processed 7000 of 10000\n",
      "Processed 8000 of 10000\n",
      "Processed 9000 of 10000\n"
     ]
    }
   ],
   "source": [
    "rawImages = []\n",
    "descHist = []\n",
    "\n",
    "count = len(train_images)\n",
    "\n",
    "for i, image in enumerate(train_images):\n",
    "    pixels = image_to_feature_vector(image)\n",
    "    histogram = extract_color_histogram(image)\n",
    "    \n",
    "    rawImages.append(pixels)\n",
    "    descHist.append(histogram)\n",
    "        \n",
    "    if i%1000 == 0: print('Processed {} of {}'.format(i, count))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from keras import utils\n",
    "\n",
    "def create_model(img_height, img_width, num_classes):\n",
    "    # Generating model\n",
    "    base_model = applications.resnet_v2.ResNet50V2(weights=None, include_top=False, input_shape=(img_height, img_width, 1))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    from keras.optimizers import SGD, Adam\n",
    "\n",
    "    adam = Adam(lr=1e-4)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
    "    earlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=10, verbose=1)\n",
    "\n",
    "    callbacks_list = [reduce_on_plateau, earlyStop]\n",
    "    callbacks = callbacks_list\n",
    "    return callbacks, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = list(train_labels.label.unique())\n",
    "\n",
    "train_data = np.array(train_images)\n",
    "train_images_gray = []\n",
    "labels_binary = []\n",
    "labels = np.array(train_labels)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    input_image = cv2.cvtColor(train_data[i], cv2.COLOR_BGR2GRAY)\n",
    "    input_image = np.reshape(input_image, (128, 128, 1))\n",
    "    train_images_gray.append(input_image)\n",
    "    \n",
    "    label = labels[i]\n",
    "    index = unique_labels.index(label)\n",
    "    \n",
    "    label = np.reshape(index, (1)) # [utils.to_categorical(index, num_classes = 10)]\n",
    "    \n",
    "    labels_binary.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: 10000\n",
      "WARNING:tensorflow:From C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "(10000, 128, 128, 1)\n",
      "(10000, 1)\n",
      "WARNING:tensorflow:From C:\\Users\\lucas\\Anaconda3\\envs\\reconhecimentopadroes\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 111s 11ms/step - loss: nan - accuracy: 2.0000e-04\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 118s 12ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      " 1760/10000 [====>.........................] - ETA: 6:37 - loss: nan - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "num_classes = 1\n",
    "\n",
    "# Image default sizes\n",
    "img_height, img_width = 128, 128\n",
    "\n",
    "print(\"X_train size: \" + str(len(rawImages)))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "callbacks, model = create_model(img_height, img_width, num_classes)\n",
    "train_images_gray = np.array(train_images_gray)\n",
    "print(train_images_gray.shape)\n",
    "\n",
    "labels_binary = np.array(labels_binary)\n",
    "print(labels_binary.shape)\n",
    "\n",
    "model.fit(train_images_gray, labels_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=True)\n",
    "\n",
    "preds = model.evaluate(X_test, y_test)\n",
    "\n",
    "model.save(\"checkpoint/model_trained_07-10.h5\")\n",
    "\n",
    "print(\"Loss = \" + str(preds[0]))\n",
    "print(\"Test Accuracy = \" + str(preds[1]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
